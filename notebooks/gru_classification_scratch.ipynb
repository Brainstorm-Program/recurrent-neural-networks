{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eecea0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.metrics as M\n",
    "from torch.utils.data import Dataset\n",
    "from scipy import integrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5041e97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, num_outputs, per_timestep_readout=True, sigma=0.01):\n",
    "        super().__init__()\n",
    "\n",
    "        self.per_timestep_readout = per_timestep_readout\n",
    "        # Gaussian random init with standard deviation *sigma*\n",
    "        init_weight = lambda *shape: nn.Parameter(torch.randn(*shape) * sigma)\n",
    "\n",
    "        # It is easier to initialize it this way since we always need to worry about\n",
    "        # (1) projections from the inputs, (2) projections from the latent state, and (3) the bias\n",
    "        # Note that unlike biological RNNs, we **do not** introduce stochasticity in the activities\n",
    " \n",
    "        triple = lambda: (init_weight(num_inputs, num_hiddens),\n",
    "                          init_weight(num_hiddens, num_hiddens),\n",
    "                          nn.Parameter(torch.zeros(num_hiddens)))\n",
    "\n",
    "        # create the parameters for the update gate\n",
    "        self.W_xz, self.W_hz, self.b_z = triple()\n",
    "\n",
    "        # create the parameters for the reset gate\n",
    "        self.W_xr, self.W_hr, self.b_r = triple()\n",
    "\n",
    "        # hidden state parameters\n",
    "        self.W_xh, self.W_hh, self.b_h = triple()\n",
    "\n",
    "        # readout layer parameters\n",
    "        self.fc = nn.Linear(num_hiddens, num_outputs)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    ''' Given that our parent class is nn.Module, what we are doing here is essentially *overloading*\n",
    "    This is the function that will be called when we pass a batch of inputs to the GRU\n",
    "    '''\n",
    "    def forward(self, inputs, H=None):\n",
    "        matmul_H = lambda A, B: torch.matmul(A, B) if H is not None else 0\n",
    "        outputs = []\n",
    "        readouts = []\n",
    "\n",
    "        for X in inputs:\n",
    "            Z = torch.sigmoid(torch.matmul(X, self.W_xz) + (\n",
    "                torch.matmul(H, self.W_hz) if H is not None else 0) + self.b_z)\n",
    "            if H is None: H = torch.zeros_like(Z)\n",
    "            R = torch.sigmoid(torch.matmul(X, self.W_xr) +\n",
    "                            torch.matmul(H, self.W_hr) + self.b_r)\n",
    "            H_tilda = torch.tanh(torch.matmul(X, self.W_xh) +\n",
    "                               torch.matmul(R * H, self.W_hh) + self.b_h)\n",
    "            H = Z * H + (1 - Z) * H_tilda\n",
    "            outputs.append(H)\n",
    "\n",
    "            if self.per_timestep_readout:\n",
    "                readouts.append(self.fc(self.relu(H)))\n",
    "\n",
    "        if not self.per_timestep_readout:\n",
    "            # final timestep readout layer\n",
    "            readouts.append(self.fc(self.relu(H)))\n",
    "\n",
    "        return outputs, readouts\n",
    "\n",
    "    def single_step(self, X, H):\n",
    "        matmul_H = lambda A, B: torch.matmul(A, B)\n",
    "\n",
    "        Z = torch.sigmoid(torch.matmul(X, self.W_xz) + (\n",
    "            torch.matmul(H, self.W_hz) if H is not None else 0) + self.b_z)\n",
    "        \n",
    "        R = torch.sigmoid(torch.matmul(X, self.W_xr) +\n",
    "                        torch.matmul(H, self.W_hr) + self.b_r)\n",
    "        \n",
    "        H_tilda = torch.tanh(torch.matmul(X, self.W_xh) +\n",
    "                           torch.matmul(R * H, self.W_hh) + self.b_h)\n",
    "        \n",
    "        H = Z * H + (1 - Z) * H_tilda\n",
    "\n",
    "        return H, self.fc(self.relu(H))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cfb708b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FitzhughNagumo(Dataset):\n",
    "    def __init__(self, N, T, I=0.5, a=0.7, b=0.8):\n",
    "        self.I = I\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.N = N\n",
    "        self.T = T\n",
    "\n",
    "        data_x = []\n",
    "        data_y = []\n",
    "        for i in range(N):\n",
    "            t = np.linspace(0,400,T+1)\n",
    "            x0 = np.array([float(np.random.rand(1))*2.-1.,0.])\n",
    "            sol = integrate.solve_ivp(self.FHN_rhs, [0,400], x0, t_eval=t)\n",
    "            data_x.append(sol.y[0,:-1])\n",
    "            data_y.append(sol.y[0,1:])\n",
    "\n",
    "        self.data_x = np.array(data_x).reshape(N,T,1)\n",
    "        self.data_y = np.array(data_y).reshape(N,T,1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.Tensor(self.data_x[idx]), torch.Tensor(self.data_y[idx])\n",
    "\n",
    "    def FHN_rhs(self, t,x):\n",
    "        I, a, b = self.I, self.a, self.b\n",
    "        eps = 1./50.\n",
    "        dim1 = x[0] - (x[0]**3)/3. - x[1] + I\n",
    "        dim2 = eps*(x[0] + a - b*x[1])\n",
    "        out = np.stack((dim1,dim2)).T\n",
    "\n",
    "        return out\n",
    "\n",
    "    def get_init(self):\n",
    "        t = np.linspace(0,400,self.T+1)\n",
    "        x0 = np.array([float(np.random.rand(1))*2.-1.,0.])\n",
    "        sol = integrate.solve_ivp(self.FHN_rhs, [0,400], x0, t_eval=t)\n",
    "        init_x = sol.y[0, :50]\n",
    "        return init_x\n",
    "\n",
    "class FitzhughNagumoClassification(Dataset):\n",
    "    def __init__(self, N, T):\n",
    "\n",
    "        # Let's sample data from two distinct dynamical systems\n",
    "        classA = FitzhughNagumo(N=N, T=T, I=0.5, a=0.95, b=0.2)\n",
    "        classB = FitzhughNagumo(N=N, T=T, I=0.5, a=0.7, b=0.2)\n",
    "       \n",
    "        # now let's create the dataset with appropriate class labels\n",
    "        self.data = torch.Tensor(np.vstack([classA.data_x, classB.data_x])).float()\n",
    "        self.labels = torch.Tensor(np.vstack([np.zeros((classA.data_x.shape[0])), np.zeros((classA.data_x.shape[0]))+1])).flatten().long()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "    def visualize_samples(self): \n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "\n",
    "        ax.plot(self.data[0, :, 0], linewidth=2, alpha=0.75, c='tab:orange', label='Class A')\n",
    "        ax.plot(self.data[-1, :, 0], linewidth=2, alpha=0.75, c='tab:brown', label='Class B')\n",
    "\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.set_xlabel('Time', fontsize=16, fontweight='bold')\n",
    "        ax.set_ylabel('Firing rate (in a.u.)', fontsize=16, fontweight='bold')\n",
    "        ax.legend(loc='upper right')\n",
    "        ax.set_xticks([0., self.data.shape[1]])\n",
    "        ax.set_xticklabels(['0ms', '{}ms'.format(self.data.shape[1])])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_ylim([-2.5, 2.5]) \n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bef460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, params, visualize_train=True):\n",
    "\n",
    "    # create the data generator to iterate over mini batches\n",
    "    trainDataGenerator = torch.utils.data.DataLoader(dataset, **params['train_params'])\n",
    "\n",
    "    # We use the cross entropy (or the negative log likelihood loss) since for this\n",
    "    # example we care about classification!\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['init_lr'])\n",
    "\n",
    "    if visualize_train:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "\n",
    "    for epoch in range(params['num_epochs']):\n",
    "\n",
    "        for data, label in trainDataGenerator:\n",
    "            # The inputs need to be of the form T x B x N_in\n",
    "            # where T is the total \"time\" duration of the signal, B is the batch size\n",
    "            # and N_in is the feature dimensionality of an observation\n",
    "            data = data.transpose(0, 1)\n",
    "\n",
    "            # forward pass\n",
    "            latent_activities, readout = model(data)\n",
    "\n",
    "            readout = readout[0]\n",
    "            # compute the loss\n",
    "            loss = criterion(readout, label)\n",
    "\n",
    "            # backpropagate through time!\n",
    "            loss.backward()\n",
    "\n",
    "            # update model parameters\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if visualize_train:\n",
    "                ax.clear()\n",
    "                # Let's pick index 0, since batch is shuffled anyway!\n",
    "                ax.plot(data[:,0,0].detach().numpy(), linewidth=2, color='tab:gray', label='groundtruth')\n",
    "                ax.plot(readout[0,:,0].detach().numpy(), '--', linewidth=2, color='r', label='prediction')\n",
    "\n",
    "                # Just formatting options. This is my pet peeve so you can safely ignore!\n",
    "                ax.spines['top'].set_visible(False)\n",
    "                ax.spines['right'].set_visible(False)\n",
    "                ax.set_title('Training epoch: {}'.format(epoch))\n",
    "                ax.set_xlabel('Time', fontsize=16, fontweight='bold')\n",
    "                ax.set_ylabel('Firing rate (in a.u.)', fontsize=16, fontweight='bold')\n",
    "                ax.legend(loc='upper right')\n",
    "                ax.set_xticks([0., data.shape[0]])\n",
    "                ax.set_xticklabels(['0ms', '{}ms'.format(data.shape[0])])\n",
    "                ax.set_yticks([])\n",
    "                ax.set_ylim([-2.5, 2.5]) \n",
    "                \n",
    "                plt.pause(0.1)\n",
    "\n",
    "        print('Epoch: {} | Training Loss: {}'.format(epoch, loss.item()))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a826b3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fhDataset = FitzhughNagumoClassification(N=128, T=1000)\n",
    "\n",
    "# If you are curious uncomment this!\n",
    "#fhDataset.visualize_samples()\n",
    "\n",
    "params = {\n",
    "        'n_inputs': 1,\n",
    "        'n_hidden': 32,\n",
    "        'num_epochs': 50,\n",
    "        'init_lr': 1e-2,\n",
    "        'n_outputs': 2,\n",
    "\n",
    "        'train_params': {\n",
    "                    'batch_size': 128,\n",
    "                    'shuffle': True,\n",
    "                    'num_workers': 1\n",
    "                }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d14e24a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model architecture and set it to train mode\n",
    "model = GRU(params['n_inputs'], params['n_hidden'], params['n_outputs'], per_timestep_readout=False)\n",
    "model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "327ce728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Training Loss: 0.6767286062240601\n",
      "Epoch: 1 | Training Loss: 0.6421626210212708\n",
      "Epoch: 2 | Training Loss: 0.6016443967819214\n",
      "Epoch: 3 | Training Loss: 0.49201899766921997\n",
      "Epoch: 4 | Training Loss: 0.4743824899196625\n",
      "Epoch: 5 | Training Loss: 0.4469873309135437\n",
      "Epoch: 6 | Training Loss: 0.3855193257331848\n",
      "Epoch: 7 | Training Loss: 0.3372969925403595\n",
      "Epoch: 8 | Training Loss: 0.33349597454071045\n",
      "Epoch: 9 | Training Loss: 0.2877713739871979\n",
      "Epoch: 10 | Training Loss: 0.8576128482818604\n",
      "Epoch: 11 | Training Loss: 0.32725968956947327\n",
      "Epoch: 12 | Training Loss: 0.3398688733577728\n",
      "Epoch: 13 | Training Loss: 0.3345402777194977\n",
      "Epoch: 14 | Training Loss: 0.30021268129348755\n",
      "Epoch: 15 | Training Loss: 0.31429311633110046\n",
      "Epoch: 16 | Training Loss: 0.26176637411117554\n",
      "Epoch: 17 | Training Loss: 0.31961143016815186\n",
      "Epoch: 18 | Training Loss: 0.302121639251709\n",
      "Epoch: 19 | Training Loss: 0.2765856683254242\n",
      "Epoch: 20 | Training Loss: 0.26387330889701843\n",
      "Epoch: 21 | Training Loss: 0.3079605996608734\n",
      "Epoch: 22 | Training Loss: 0.2041899710893631\n",
      "Epoch: 23 | Training Loss: 0.26772385835647583\n",
      "Epoch: 24 | Training Loss: 0.26506686210632324\n",
      "Epoch: 25 | Training Loss: 0.2643916606903076\n",
      "Epoch: 26 | Training Loss: 0.25446784496307373\n",
      "Epoch: 27 | Training Loss: 0.24012674391269684\n",
      "Epoch: 28 | Training Loss: 0.20448075234889984\n",
      "Epoch: 29 | Training Loss: 0.17993435263633728\n",
      "Epoch: 30 | Training Loss: 0.2066715657711029\n",
      "Epoch: 31 | Training Loss: 0.23226869106292725\n",
      "Epoch: 32 | Training Loss: 0.1859147697687149\n",
      "Epoch: 33 | Training Loss: 0.16330423951148987\n",
      "Epoch: 34 | Training Loss: 0.17191050946712494\n",
      "Epoch: 35 | Training Loss: 0.12751401960849762\n",
      "Epoch: 36 | Training Loss: 0.10084620863199234\n",
      "Epoch: 37 | Training Loss: 0.07665152102708817\n",
      "Epoch: 38 | Training Loss: 0.07279866188764572\n",
      "Epoch: 39 | Training Loss: 0.06708657741546631\n",
      "Epoch: 40 | Training Loss: 0.05329874902963638\n",
      "Epoch: 41 | Training Loss: 0.04263138025999069\n",
      "Epoch: 42 | Training Loss: 0.03439614549279213\n",
      "Epoch: 43 | Training Loss: 0.026822557672858238\n",
      "Epoch: 44 | Training Loss: 0.02284112200140953\n",
      "Epoch: 45 | Training Loss: 0.020531192421913147\n",
      "Epoch: 46 | Training Loss: 0.01670668087899685\n",
      "Epoch: 47 | Training Loss: 0.013408081606030464\n",
      "Epoch: 48 | Training Loss: 0.01143819373100996\n",
      "Epoch: 49 | Training Loss: 0.00761529803276062\n"
     ]
    }
   ],
   "source": [
    "# Now let's train the model. \n",
    "# Pass visualize_train=False to suppress any display\n",
    "model = train_model(model, fhDataset, params, visualize_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2649c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    # First off, let's create a new dataset. Since the initializations are random, we can \n",
    "    # consider this a proper test! To make life harder for the model, lets change up T too\n",
    "    N, T = 128, 2000\n",
    "    test_dataset = FitzhughNagumoClassification(N=N, T=T)\n",
    " \n",
    "    # Create the data tensors\n",
    "    x = test_dataset.data.permute(1,0,-1)\n",
    "    y = test_dataset.labels\n",
    "\n",
    "    # Compute the feedforward pass. \n",
    "    # But since we aren't training, we can do without the gradients\n",
    "    with torch.no_grad():\n",
    "        _, pred = model(x)\n",
    "        pred = torch.argmax(pred[0], dim=-1)\n",
    "\n",
    "    # Let's create the confusion matrix\n",
    "    cm = M.confusion_matrix(y.numpy(), pred.numpy()).astype(np.float32)\n",
    "    # Compute accuracy\n",
    "    print('Overall accuracy: {}'.format((cm[0,0]+cm[1,1])/cm.sum()))\n",
    "\n",
    "    # row normalize\n",
    "    cm[0] = cm[0]/cm[0].sum()\n",
    "    cm[1] = cm[1]/cm[1].sum()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(cm, cmap=plt.get_cmap('YlGn'), vmin=0., vmax=1.)\n",
    "\n",
    "    for y in range(cm.shape[0]):\n",
    "        for x in range(cm.shape[1]):\n",
    "            ax.text(x, y, '%.2f' % cm[y, x],\n",
    "                 horizontalalignment='center',\n",
    "                 verticalalignment='center',\n",
    "                 fontsize=12,\n",
    "                 color='white',\n",
    "                 fontweight='bold'\n",
    "                 )\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    ax.set_ylabel('True label', fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('Predicted label', fontsize=16, fontweight='bold')\n",
    "    #plt.savefig('thumbs/cfmat.png', bbox_inches='tight')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "368ab502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's set the model to eval mode, and see its performance on a new random set\n",
    "model = model.eval()\n",
    "evaluate_model(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
