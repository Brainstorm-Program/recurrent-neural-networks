{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95b50817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from copy import deepcopy\n",
    "from scipy import integrate\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf6700e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, num_outputs, per_timestep_readout=True, sigma=0.01):\n",
    "        super().__init__()\n",
    "\n",
    "        self.per_timestep_readout = per_timestep_readout\n",
    "        # Gaussian random init with standard deviation *sigma*\n",
    "        init_weight = lambda *shape: nn.Parameter(torch.randn(*shape) * sigma)\n",
    "\n",
    "        # It is easier to initialize it this way since we always need to worry about\n",
    "        # (1) projections from the inputs, (2) projections from the latent state, and (3) the bias\n",
    "        # Note that unlike biological RNNs, we **do not** introduce stochasticity in the activities\n",
    " \n",
    "        triple = lambda: (init_weight(num_inputs, num_hiddens),\n",
    "                          init_weight(num_hiddens, num_hiddens),\n",
    "                          nn.Parameter(torch.zeros(num_hiddens)))\n",
    "\n",
    "        # create the parameters for the update gate\n",
    "        self.W_xz, self.W_hz, self.b_z = triple()\n",
    "\n",
    "        # create the parameters for the reset gate\n",
    "        self.W_xr, self.W_hr, self.b_r = triple()\n",
    "\n",
    "        # hidden state parameters\n",
    "        self.W_xh, self.W_hh, self.b_h = triple()\n",
    "\n",
    "        # readout layer parameters\n",
    "        self.fc = nn.Linear(num_hiddens, num_outputs)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    ''' Given that our parent class is nn.Module, what we are doing here is essentially *overloading*\n",
    "    This is the function that will be called when we pass a batch of inputs to the GRU\n",
    "    '''\n",
    "    def forward(self, inputs, H=None):\n",
    "        matmul_H = lambda A, B: torch.matmul(A, B) if H is not None else 0\n",
    "        outputs = []\n",
    "        readouts = []\n",
    "\n",
    "        for X in inputs:\n",
    "            Z = torch.sigmoid(torch.matmul(X, self.W_xz) + (\n",
    "                torch.matmul(H, self.W_hz) if H is not None else 0) + self.b_z)\n",
    "            if H is None: H = torch.zeros_like(Z)\n",
    "            R = torch.sigmoid(torch.matmul(X, self.W_xr) +\n",
    "                            torch.matmul(H, self.W_hr) + self.b_r)\n",
    "            H_tilda = torch.tanh(torch.matmul(X, self.W_xh) +\n",
    "                               torch.matmul(R * H, self.W_hh) + self.b_h)\n",
    "            H = Z * H + (1 - Z) * H_tilda\n",
    "            outputs.append(H)\n",
    "\n",
    "            if self.per_timestep_readout:\n",
    "                readouts.append(self.fc(self.relu(H)))\n",
    "\n",
    "        if not self.per_timestep_readout:\n",
    "            # final timestep readout layer\n",
    "            readouts.append(self.fc(self.relu(H)))\n",
    "\n",
    "        return outputs, readouts\n",
    "\n",
    "    def single_step(self, X, H):\n",
    "        matmul_H = lambda A, B: torch.matmul(A, B)\n",
    "\n",
    "        Z = torch.sigmoid(torch.matmul(X, self.W_xz) + (\n",
    "            torch.matmul(H, self.W_hz) if H is not None else 0) + self.b_z)\n",
    "        \n",
    "        R = torch.sigmoid(torch.matmul(X, self.W_xr) +\n",
    "                        torch.matmul(H, self.W_hr) + self.b_r)\n",
    "        \n",
    "        H_tilda = torch.tanh(torch.matmul(X, self.W_xh) +\n",
    "                           torch.matmul(R * H, self.W_hh) + self.b_h)\n",
    "        \n",
    "        H = Z * H + (1 - Z) * H_tilda\n",
    "\n",
    "        return H, self.fc(self.relu(H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9d76aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FitzhughNagumo(Dataset):\n",
    "    def __init__(self, N, T, I=0.5, a=0.7, b=0.8):\n",
    "        self.I = I\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.N = N\n",
    "        self.T = T\n",
    "\n",
    "        data_x = []\n",
    "        data_y = []\n",
    "        for i in range(N):\n",
    "            t = np.linspace(0,400,T+1)\n",
    "            x0 = np.array([float(np.random.rand(1))*2.-1.,0.])\n",
    "            sol = integrate.solve_ivp(self.FHN_rhs, [0,400], x0, t_eval=t)\n",
    "            data_x.append(sol.y[0,:-1])\n",
    "            data_y.append(sol.y[0,1:])\n",
    "\n",
    "        self.data_x = np.array(data_x).reshape(N,T,1)\n",
    "        self.data_y = np.array(data_y).reshape(N,T,1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.Tensor(self.data_x[idx]), torch.Tensor(self.data_y[idx])\n",
    "\n",
    "    def FHN_rhs(self, t,x):\n",
    "        I, a, b = self.I, self.a, self.b\n",
    "        eps = 1./50.\n",
    "        dim1 = x[0] - (x[0]**3)/3. - x[1] + I\n",
    "        dim2 = eps*(x[0] + a - b*x[1])\n",
    "        out = np.stack((dim1,dim2)).T\n",
    "\n",
    "        return out\n",
    "\n",
    "    def get_init(self):\n",
    "        t = np.linspace(0,400,self.T+1)\n",
    "        x0 = np.array([float(np.random.rand(1))*2.-1.,0.])\n",
    "        sol = integrate.solve_ivp(self.FHN_rhs, [0,400], x0, t_eval=t)\n",
    "        init_x = sol.y[0, :50]\n",
    "        return init_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43637e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, params, warm_up=50):\n",
    "\n",
    "    # create the data generator to iterate over mini batches\n",
    "    trainDataGenerator = torch.utils.data.DataLoader(dataset, **params['train_params'])\n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['init_lr'])\n",
    "\n",
    "    for epoch in range(params['num_epochs']):\n",
    "\n",
    "        for data, label in trainDataGenerator:\n",
    "            # The inputs need to be of the form T x B x N_in\n",
    "            # where T is the total \"time\" duration of the signal, B is the batch size\n",
    "            # and N_in is the feature dimensionality of an observation\n",
    "            data = data.transpose(0, 1)\n",
    "\n",
    "            # forward pass to warm-up\n",
    "            latent_activities, readout = model(data[:warm_up])\n",
    "\n",
    "            # now the autoregression begins\n",
    "            autoreg_outputs = []\n",
    "            latent = latent_activities[-1]\n",
    "            X = readout[-1]\n",
    "\n",
    "            for t in range(warm_up, data.shape[0]):\n",
    "                latent, X = model.single_step(X, latent)\n",
    "                autoreg_outputs.append(X)\n",
    "\n",
    "            autoreg_outputs = torch.stack(autoreg_outputs)\n",
    "\n",
    "            # compute the loss\n",
    "            loss = criterion(autoreg_outputs, data[warm_up:]) #.to('cuda:0'))\n",
    "\n",
    "            # backpropagate through time!\n",
    "            loss.backward()\n",
    "\n",
    "            # update model parameters\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        print('Epoch: {} | Training Loss: {}'.format(epoch, loss.item()))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8cfe766",
   "metadata": {},
   "outputs": [],
   "source": [
    "fhDataset = FitzhughNagumo(N=256, T=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18b97602",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'n_inputs': 1,\n",
    "        'n_hidden': 128,\n",
    "        'num_epochs': 1000,\n",
    "        'init_lr': 1e-3,\n",
    "        'n_outputs': 1,\n",
    "\n",
    "        'train_params': {\n",
    "                    'batch_size': 128,\n",
    "                    'shuffle': True,\n",
    "                    'num_workers': 1\n",
    "                }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd280f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model architecture and set it to train mode\n",
    "model = GRU(params['n_inputs'], params['n_hidden'], params['n_outputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e3e22d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join('../ckpts', 'autoregressiveGRU.pth')):\n",
    "    model = model.train()\n",
    "\n",
    "    # Now let's train the model. \n",
    "    # Pass visualize_train=False to suppress any display\n",
    "    model = train_model(model, fhDataset, params, visualize_train=False)\n",
    "    torch.save(model.state_dict(), '../ckpts/autoregressiveGRU.pth')\n",
    "\n",
    "else:\n",
    "    model.load_state_dict(torch.load(os.path.join('../ckpts', 'autoregressiveGRU.pth')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bb29ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, init_x, future_T=1000):\n",
    "    model = model.eval()\n",
    "    gen_seq = init_x.clone()\n",
    "\n",
    "    for t in tqdm.tqdm(range(future_T)):\n",
    "        with torch.no_grad():\n",
    "            _, output = model(gen_seq)\n",
    "            gen_seq = torch.cat([gen_seq, output[-1].unsqueeze(0)], dim=0) \n",
    "\n",
    "    return gen_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d7fd8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:05<00:00, 42.81it/s]\n"
     ]
    }
   ],
   "source": [
    "model = model.eval()\n",
    "\n",
    "# This is going to be cool. We can treat RNNs as \"generative\" models too :)\n",
    "# Let's \"seed\" the model with an initial sequence\n",
    "init_x = fhDataset.get_init()\n",
    "init_x = torch.Tensor(init_x[:, np.newaxis, np.newaxis])\n",
    "\n",
    "gen_seq = generate(model, init_x, future_T=250)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
